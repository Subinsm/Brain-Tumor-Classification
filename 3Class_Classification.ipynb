{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop \n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, Conv3D, MaxPooling3D, Input, concatenate, AveragePooling3D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"/content/drive/My Drive/Upenn_Data/training_data.csv\") #refer .csv for cross-validation implementation\n",
    "df_val = pd.read_csv(r\"/content/drive/My Drive/Upenn_Data/validation_data.csv\")\n",
    "column = \"CPM_RadPath_2019_ID\"\n",
    "class_name = \"class\"\n",
    "input_path = r'/content/drive/My Drive/UPenn_Data/miccai2019-data/CPM-RadPath_2019_Training_Data/Radiology'\n",
    "input_path_pat = r'/content/drive/My Drive/Pathology/Output'\n",
    "flair_format = \"_flair.nii.gz\"\n",
    "t1_format = \"_t1.nii.gz\"\n",
    "t1ce_format = \"_t1ce.nii.gz\"\n",
    "t2_format = \"_t2.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flair = []\n",
    "val_flair = []\n",
    "train_t1 = []\n",
    "val_t1 = []\n",
    "train_t1ce = []\n",
    "val_t1ce = []\n",
    "train_t2 = []\n",
    "val_t2 = []\n",
    "train_pat = []\n",
    "val_pat = []\n",
    "class_train = []\n",
    "class_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,208):\n",
    "    data_path = df_train[column][i]\n",
    "    \n",
    "    flair_path = os.path.join(input_path,data_path,data_path+flair_format)\n",
    "    flair_image = nib.load(flair_path)\n",
    "    flair = flair_image.get_data()\n",
    "    flair[flair!=0] = ((((flair[flair!=0] - np.mean(flair[flair!=0]))/ np.std(flair[flair!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    train_flair.append(flair)\n",
    "    \n",
    "    t1_path = os.path.join(input_path,data_path,data_path+t1_format)\n",
    "    t1_image = nib.load(t1_path)\n",
    "    t1 = t1_image.get_data()\n",
    "    t1[t1!=0] = ((((t1[t1!=0] - np.mean(t1[t1!=0]))/ np.std(t1[t1!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    train_t1.append(t1)\n",
    "    \n",
    "    t1ce_path = os.path.join(input_path,data_path,data_path+t1ce_format)\n",
    "    t1ce_image = nib.load(t1ce_path)\n",
    "    t1ce = t1ce_image.get_data()\n",
    "    t1ce[t1ce!=0] = ((((t1ce[t1ce!=0] - np.mean(t1ce[t1ce!=0]))/ np.std(t1ce[t1ce!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    train_t1ce.append(t1ce)\n",
    "    \n",
    "    t2_path = os.path.join(input_path,data_path,data_path+t2_format)\n",
    "    t2_image = nib.load(t2_path)\n",
    "    t2 = t2_image.get_data()\n",
    "    t2[t2!=0] = ((((t2[t2!=0] - np.mean(t2[t2!=0]))/ np.std(t2[t2!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    train_t2.append(t2)\n",
    "    \n",
    "    train_class.append(df_train[class_name][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,52):\n",
    "    data_path = df_val[column][i]\n",
    "    \n",
    "    flair_path = os.path.join(input_path,data_path,data_path+flair_format)\n",
    "    flair_image = nib.load(flair_path)\n",
    "    flair = flair_image.get_data()\n",
    "    flair[flair!=0] = ((((flair[flair!=0] - np.mean(flair[flair!=0]))/ np.std(flair[flair!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    val_flair.append(flair)\n",
    "    \n",
    "    t1_path = os.path.join(input_path,data_path,data_path+t1_format)\n",
    "    t1_image = nib.load(t1_path)\n",
    "    t1 = t1_image.get_data()\n",
    "    t1[t1!=0] = ((((t1[t1!=0] - np.mean(t1[t1!=0]))/ np.std(t1[t1!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    val_t1.append(t1)\n",
    "    \n",
    "    t1ce_path = os.path.join(input_path,data_path,data_path+t1ce_format)\n",
    "    t1ce_image = nib.load(t1ce_path)\n",
    "    t1ce = t1ce_image.get_data()\n",
    "    t1ce[t1ce!=0] = ((((t1ce[t1ce!=0] - np.mean(t1ce[t1ce!=0]))/ np.std(t1ce[t1ce!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    val_t1ce.append(t1ce)\n",
    "    \n",
    "    t2_path = os.path.join(input_path,data_path,data_path+t2_format)\n",
    "    t2_image = nib.load(t2_path)\n",
    "    t2 = t2_image.get_data()\n",
    "    t2[t2!=0] = ((((t2[t2!=0] - np.mean(t2[t2!=0]))/ np.std(t2[t2!=0]))+ 10)* 100).astype(dtype=np.int16)\n",
    "    val_t2.append(t2)\n",
    "    \n",
    "    val_class.append(df_val[class_name][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,208):\n",
    "    df_path = df_train[column][i]\n",
    "    \n",
    "    patch_list = open(os.path.join(input_path_pat,df_path,\"patch_list.pkl\"), \"rb\")\n",
    "    sorted_fsd = pickle.load(patch_list)\n",
    "    patch_list.close()\n",
    "    temp=[]\n",
    "    count = 0\n",
    "    for i in sorted_fsd:\n",
    "        image = cv2.imread(os.path.join(input_path_pat,df_path,df_path+\"_\"+str(sorted_fsd[i][0][0])+\"_\"+str(sorted_fsd[i][0][1])+\".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "        new_image = cv2.resize(image, (224,224))\n",
    "        temp.append(new_image)\n",
    "        count=+1\n",
    "        if count >= 8:\n",
    "            break\n",
    "    \n",
    "    temp = np.array(temp).reshape(8,224,224)\n",
    "    val_pat.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,52):\n",
    "    df_path = df_val[column][i]\n",
    "    \n",
    "    patch_list = open(os.path.join(input_path_pat,df_path,\"patch_list.pkl\"), \"rb\")\n",
    "    sorted_fsd = pickle.load(patch_list)\n",
    "    patch_list.close()\n",
    "    temp=[]\n",
    "    count = 0\n",
    "    for i in sorted_fsd:\n",
    "        image = cv2.imread(os.path.join(input_path_pat,df_path,df_path+\"_\"+str(sorted_fsd[i][0][0])+\"_\"+str(sorted_fsd[i][0][1])+\".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "        new_image = cv2.resize(image, (224,224))\n",
    "        temp.append(new_image)\n",
    "        count=+1\n",
    "        if count >= 8:\n",
    "            break\n",
    "    \n",
    "    temp = np.array(temp).reshape(8,224,224)\n",
    "    train_pat.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = []\n",
    "val_class = []\n",
    "\n",
    "for i in class_train:\n",
    "    if i == 'A':\n",
    "        train_class.append(0)\n",
    "    elif i == 'O':\n",
    "        train_class.append(1)\n",
    "    else:\n",
    "        train_class.append(2)\n",
    "\n",
    "for i in class_val:\n",
    "    if i == 'A':\n",
    "        val_class.append(0)\n",
    "    elif i == 'O':\n",
    "        val_class.append(1)\n",
    "    else:\n",
    "        val_class.append(2)\n",
    "\n",
    "train_labels = to_categorical(train_class)\n",
    "val_labels = to_categorical(val_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flair = np.array(train_flair)\n",
    "val_flair = np.array(val_flair)\n",
    "train_t1 = np.array(train_t1)\n",
    "val_t1 = np.array(val_t1)\n",
    "train_t1ce = np.array(train_t1ce)\n",
    "val_t1ce = np.array(val_t1ce)\n",
    "train_t2 = np.array(train_t2)\n",
    "val_t2 = np.array(val_t2)\n",
    "\n",
    "train_flair = np.array(train_flair).reshape(208, 80, 96, 64, 1)\n",
    "val_flair = np.array(val_flair).reshape(52, 80, 96, 64, 1)\n",
    "train_t1 = np.array(train_t1).reshape(208, 80, 96, 64, 1)\n",
    "val_t1 = np.array(val_t1).reshape(52, 80, 96, 64, 1)\n",
    "train_t1ce = np.array(train_t1ce).reshape(208, 80, 96, 64, 1)\n",
    "val_t1ce = np.array(val_t1ce).reshape(52, 80, 96, 64, 1)\n",
    "train_t2 = np.array(train_t2).reshape(208, 80, 96, 64, 1)\n",
    "val_t2 = np.array(val_t2).reshape(52, 80, 96, 64, 1)\n",
    "train_pat = np.array(train_pat).reshape(208,8,224,224,1)\n",
    "val_pat = np.array(val_pat).reshape(52,8,224,224,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape1 = Input(shape = (80, 96, 64, 1))\n",
    "input_shape2 = Input(shape = (80, 96, 64, 1))\n",
    "input_shape3 = Input(shape = (80, 96, 64, 1))\n",
    "input_shape4 = Input(shape = (80, 96, 64, 1))\n",
    "input_shape5 = Input(shape = (8,224,224,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv3D(32, 3, activation = 'relu')(input_shape1)\n",
    "pool1 = MaxPooling3D(pool_size = (3, 3, 3))(conv1)\n",
    "flat1 = Flatten()(pool1)\n",
    "\n",
    "conv2 = Conv3D(32, 3, activation = 'relu')(input_shape2)\n",
    "pool2 = MaxPooling3D(pool_size = (3, 3, 3))(conv2)\n",
    "flat2 = Flatten()(pool2)\n",
    "\n",
    "conv3 = Conv3D(32, 3, activation = 'relu')(input_shape3)\n",
    "pool3 = MaxPooling3D(pool_size = (3, 3, 3))(conv3)\n",
    "flat3 = Flatten()(pool3)\n",
    "\n",
    "conv4 = Conv3D(32, 3, activation = 'relu')(input_shape4)\n",
    "pool4 = MaxPooling3D(pool_size = (3, 3, 3))(conv4)\n",
    "flat4 = Flatten()(pool4)\n",
    "\n",
    "pConv1 = Conv3D(64, 7, activation = 'relu')(input_shape5)\n",
    "pConv2 = Conv3D(64, 3, activation = 'relu')(pConv1)\n",
    "pConv3 = Conv3D(64, 3, activation = 'relu')(pConv2)\n",
    "pConv4 = Conv3D(64, 3, activation = 'relu')(pConv3)\n",
    "pConv5 = Conv3D(64, 3, activation = 'relu')(pConv4)\n",
    "pConv6 = Conv3D(128, 3, activation = 'relu')(pConv5)\n",
    "pConv7 = Conv3D(128, 3, activation = 'relu')(pConv6)\n",
    "pConv8 = Conv3D(128, 3, activation = 'relu')(pConv7)\n",
    "pConv9 = Conv3D(128, 3, activation = 'relu')(pConv8)\n",
    "pConv10 = Conv3D(256, 3, activation = 'relu')(pConv9)\n",
    "pConv11 = Conv3D(256, 3, activation = 'relu')(pConv10)\n",
    "pConv12 = Conv3D(256, 3, activation = 'relu')(pConv11)\n",
    "pConv13 = Conv3D(256, 3, activation = 'relu')(pConv12)\n",
    "pConv14 = Conv3D(512, 3, activation = 'relu')(pConv13)\n",
    "pConv15 = Conv3D(512, 3, activation = 'relu')(pConv14)\n",
    "pConv16 = Conv3D(512, 3, activation = 'relu')(pConv15)\n",
    "pConv17 = Conv3D(512, 3, activation = 'relu')(pConv16)\n",
    "pool5 = AveragePooling3D(pool_size = (3, 3, 3))(pConv17)\n",
    "flat5 = Flatten()(pool5)\n",
    "\n",
    "merge = concatenate([flat1,flat2,flat3,flat4,flat5])\n",
    "hidden = Dense(128, activation = 'relu')(merge)\n",
    "dropout = Dropout(0.5)(hidden)\n",
    "output = Dense(3, activation = 'softmax')(dropout)\n",
    "\n",
    "model = Model(inputs = [input_shape1,input_shape2,input_shape3,input_shape4,input_shape5], outputs = output)\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = Adam(lr = 0.0001),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timee = time.time()\n",
    "\n",
    "history = model.fit([train_flair,train_t1,train_t1ce,train_t2,train_pat], train_labels, epochs = 20, batch_size = 8)\n",
    "\n",
    "print(time.time()-timee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/Upenn_Data/Models/3class_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate([val_flair,val_t1,val_t1ce,val_t2,val_pat], val_labels, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Accuracy', 'Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
